<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://pdxlabs.io/blog</id>
    <title>PDX Blog</title>
    <updated>2023-07-01T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://pdxlabs.io/blog"/>
    <subtitle>PDX Blog</subtitle>
    <icon>https://pdxlabs.io/img/favicon.png</icon>
    <entry>
        <title type="html"><![CDATA[Why build PDX?]]></title>
        <id>https://pdxlabs.io/blog/why-build-pdx</id>
        <link href="https://pdxlabs.io/blog/why-build-pdx"/>
        <updated>2023-07-01T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[A framework for building and managing applications powered by Large Language Models LLMs]]></summary>
        <content type="html"><![CDATA[<p>In the last 6 months, I've been building a lot of applications that are being powered by LLMs. And, in this time, none of the available frameworks helped me satisfy the requirements I had. Eventually, I had to write a lot of scaffold code that I reused across all these applications. PDX is a consolidation of all that scaffold code, which I find very useful. And, hopefully to you too...</p><p>Before I start preaching about PDX, let me start with what the initial requirements were:</p><ol><li>Low on dependencies. You're just making an call to an API with some text.</li><li>Managing prompt templates with Jinja. Jinja is a great templating engine, which is well architected for the use-cases one might encounter.</li><li>Separating prompt templates with the application code. Prompt templates are plaint text, and it has no business being written into the application code.</li><li>Chaining of prompts. Both text and chat completions need a well thought of way of sequencing the prompts.</li><li>Observability. Logging which prompts with which inputs gave which response from which model. Along with how long did it take and how many tokens were used.</li><li>A/B testing. Attributing the the above with the user feedback.</li><li>Caching responses. Separating the prompts as templates and inputs for those templates makes it easier for caching. Instead of looking up the entire prompt, only the inputs need to be looked up for a response.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-is-pdx">What is PDX?<a href="#what-is-pdx" class="hash-link" aria-label="Direct link to What is PDX?" title="Direct link to What is PDX?">​</a></h2><p>PDX is a prompt engineering and a dev-ops toolkit. At the core, it provides a framework on how to build and manage agents. An agent is an interface to the lanugage models, which is a collection of prompts and/or prompt templates with information that is used as input to the Language Models.</p><blockquote><p>Disambiguation: Some frameworks use the word Agent to describe an autonomous agent that perform a set of actions by interacting with the LLMs.</p></blockquote><h3 class="anchor anchorWithStickyNavbar_LWe7" id="folder-structure--config">Folder structure + config<a href="#folder-structure--config" class="hash-link" aria-label="Direct link to Folder structure + config" title="Direct link to Folder structure + config">​</a></h3><p>PDX borrows from <a href="https://flask.palletsprojects.com/en/2.3.x/blueprints/#" target="_blank" rel="noopener noreferrer">Flask-blueprints</a> the concept of modularising applications. Applications built to use LLMs, employ multiple different agents. And, PDX helps in modularising applications with folder-structures to build and manage agents.</p><blockquote><p>For example, an application that creates an itinerary would perhaps have one agent to create a draft itinerary with the inputs from the user. Which would pass the response to another agent that checks the itinerary for factual errors, another one that checks for correct formatting of timing and so on. With PDX, each of these agents would have a folder for each of them with just a configuration file and their respective templates.</p></blockquote><p>A typical agent folder would look like:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#F8F8F2;--prism-background-color:#282A36"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#F8F8F2"><span class="token plain">demo_agent</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── __init__.py</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── config.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    ├── templates</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │&nbsp;&nbsp; ├── 1_prompt.defaults.yaml</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    │&nbsp;&nbsp; └── 1_prompt.jinja</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">    └── tests</span><br></span><span class="token-line" style="color:#F8F8F2"><span class="token plain">        └── test_1.yaml</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>With a folder for the prompt templates along with the default values of the template fields. A <a href="/docs/getting-started/main-concepts#agent-configuration">configuration file</a> for info on the prompt template, the model to use and so on. More info on this can be found <a href="/docs/getting-started/main-concepts#agent-configuration">here</a></p><p>Along with this a folder (<code>/tests</code>) contains tests for the agent, that can be used to keep track of the agent's performance.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="prompt-templates">Prompt templates<a href="#prompt-templates" class="hash-link" aria-label="Direct link to Prompt templates" title="Direct link to Prompt templates">​</a></h3><p>Prompts to Language Models are text. A great way to reuse parts of this text prompt is to use templates, and <a href="https://jinja.palletsprojects.com/en/3.1.x/" target="_blank" rel="noopener noreferrer">Jinja</a> happens to be a great templating engine. Templates in PDX are written as Jinja tempaltes.</p><p>More often than not, the responses from the agents are non-deterministic. Thus when making changes to the prompts in the hope that they would perform better with their response, the changes need to be tested and evaluated. Maintaining the prompts for the agents separated from the application code has the advantage that you can version the prompts and some evaluation metrics for those prompts. This particularly helps when making changes to prompts, you could compare the performance to other versions of the prompts of the agent.</p><p>Another important aspect to this structure is caching. Instead of caching the entire prompt to the model and its response, we could optimize this by storing only the input to the prompt templates and its response. This eliminates the redundant template text for being stored and processed for each call.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability">Observability<a href="#observability" class="hash-link" aria-label="Direct link to Observability" title="Direct link to Observability">​</a></h3><p>For each agent execution, PDX provides a method to extract the meta information of the execution. This meta information includes prompt version, prompt template information, the input values to the agent, the model and the parameters used, the response, time taken to recieve the response and information on token usage.</p><p>This provides the ease of logging into any of the analytics tools.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="concluding-thoughts">Concluding thoughts<a href="#concluding-thoughts" class="hash-link" aria-label="Direct link to Concluding thoughts" title="Direct link to Concluding thoughts">​</a></h2><p>This is mental model I've used for the projects I've worked on. One might say this a highly opinionated way of building interfaces to language models. Yes, it is and I hope it works for your use-case as well.</p>]]></content>
        <author>
            <name>Adithya Krishnan</name>
            <uri>https://krishadi.com</uri>
        </author>
        <category label="llm" term="llm"/>
        <category label="apps" term="apps"/>
    </entry>
</feed>